# Gradient Descent

repeat until convergence {

$\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1) \text{   (for j = 0 and j = 1)}$

}

$\alpha$ = learning rate (positive number)

$\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1)$ = the slope
